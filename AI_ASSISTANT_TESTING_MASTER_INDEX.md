# AI Assistant Testing - Master Index & Implementation Guide

**Created**: November 5, 2025  
**Purpose**: Comprehensive guide to testing your AI Assistant's capabilities

---

## üìö DOCUMENTATION PROVIDED

You now have **4 comprehensive testing documents**:

### 1. **AI_ASSISTANT_TEST_GUIDE_COMPREHENSIVE.md** (Main Guide)
- **What**: 83 detailed test questions organized by complexity and domain
- **Length**: ~400 lines, very detailed
- **Best for**: Complete, systematic testing with full context
- **Structure**:
  - Part 1: Data Retrieval Questions (44 questions)
  - Part 2: Strategic Questions - RAG Capability (39 questions)
  - Quality metrics and troubleshooting

**Start here if**: You want the most detailed, thorough testing approach

---

### 2. **AI_ASSISTANT_TESTING_QUICK_REFERENCE.md** (Quick Start)
- **What**: Summary of testing approach with quick verification tests
- **Length**: ~250 lines, condensed
- **Best for**: Quick overview and 5-minute verification tests
- **Key sections**:
  - Overview of 83 questions
  - Quick verification tests (5 min each)
  - Expected capabilities checklist
  - Testing metrics

**Start here if**: You want a fast overview before diving into detailed testing

---

### 3. **AI_ASSISTANT_TESTING_EXAMPLES.md** (Learning Examples)
- **What**: Realistic example Q&A with evaluation guidance
- **Length**: ~350 lines with detailed examples
- **Best for**: Understanding what good vs bad responses look like
- **Includes**:
  - 6 detailed example walkthroughs
  - Expected vs actual responses
  - How to recognize when RAG is working
  - Response evaluation checklist

**Start here if**: You want to calibrate expectations and learn what to look for

---

### 4. **AI_ASSISTANT_TESTING_QUICK_REFERENCE.md** (This file - Master Index)
- **What**: Navigation guide and quick implementation checklist
- **Best for**: Finding what you need and getting started
- **Use**: As reference to jump to other documents

---

## üéØ QUICK START (5 MINUTES)

### If you have 5 minutes:
1. Read this section (you're doing it now!)
2. Open `AI_ASSISTANT_TESTING_QUICK_REFERENCE.md`
3. Check the "Quick Verification Tests" section
4. Ask your AI the 5 quick data retrieval questions
5. Ask your AI the 3 quick RAG questions
6. See if basic capabilities work

### If you have 30 minutes:
1. Read `AI_ASSISTANT_TESTING_QUICK_REFERENCE.md` (entire)
2. Read `AI_ASSISTANT_TESTING_EXAMPLES.md` Examples 1-3
3. Start testing with categories 1A and 1B from the comprehensive guide
4. Note what works and what doesn't

### If you have 2+ hours:
1. Read all documentation in this order:
   - `AI_ASSISTANT_TESTING_QUICK_REFERENCE.md`
   - `AI_ASSISTANT_TESTING_EXAMPLES.md`
   - `AI_ASSISTANT_TEST_GUIDE_COMPREHENSIVE.md`
2. Systematically test all 83 questions
3. Create a detailed test report
4. Identify improvement areas

---

## üìã QUESTION CATEGORIES AT A GLANCE

### PART 1: DATA RETRIEVAL (44 Questions)
These test if the AI can accurately fetch and report your project data.

| Category | Questions | Topics | Difficulty |
|----------|-----------|--------|-----------|
| **Organizations** | 1-7 | Org listing, counting, comparisons | Beginner ‚Üí Advanced |
| **Boards** | 8-16 | Board data, task counts, teams | Beginner ‚Üí Advanced |
| **Tasks** | 17-28 | Task filtering, status, dependencies | Beginner ‚Üí Advanced |
| **Teams & Resources** | 29-36 | Team composition, workload, capacity | Beginner ‚Üí Advanced |
| **Risk & Status** | 37-44 | Risk identification, indicators, analysis | Beginner ‚Üí Advanced |

**Goal**: Verify AI can accurately retrieve and report on your project data

---

### PART 2: STRATEGIC QUESTIONS (39 Questions)
These test if the AI can combine your project data with best practices (RAG capability).

| Category | Questions | Topics | Focus |
|----------|-----------|--------|-------|
| **Risk Strategies** | 45-53 | How to handle risks, mitigation | Data + Best Practices |
| **PM Strategy** | 54-61 | Project delivery, team optimization | Your Projects + Frameworks |
| **Organizational** | 62-66 | Org structure, alignment, scaling | Your Structure + Strategy |
| **Process Improvement** | 67-72 | Metrics, delays, communication | Your Current State + Improvements |
| **Strategic Decisions** | 73-78 | Hiring, prioritization, tool choices | Your Constraints + Decisions |
| **Trends & Forecasting** | 79-83 | Market trends, forecasts, planning | Your Data + External Trends |

**Goal**: Verify AI blends YOUR specific data with general best practices to give tailored advice

---

## üöÄ TESTING WORKFLOW

### Phase 1: Setup (10 minutes)
```
‚úÖ Ensure AI assistant is running
‚úÖ Open chat interface
‚úÖ Load demo data (if not already loaded)
‚úÖ Have a test user logged in
‚úÖ Open these documents in reference window
```

### Phase 2: Data Retrieval Testing (30-45 minutes)
```
Questions to ask: 1-44 (organized by category)
For each question:
  1. Ask the question
  2. Check accuracy against your actual project data
  3. Rate: ‚úÖ Correct, ‚ö†Ô∏è Partial, ‚ùå Wrong
  4. Note response time and any issues
```

### Phase 3: RAG Testing (45-60 minutes)
```
Questions to ask: 45-83 (organized by strategy type)
For each question look for:
  üåê Web search mention? (Industry research, trends, best practices)
  üìä Your data reference? (Your specific tasks, teams, metrics)
  üß™ Good synthesis? (Blended together vs separate lists)
  ‚ö° Actionable? (Can you actually implement the advice?)
```

### Phase 4: Report & Analysis (15-20 minutes)
```
Document findings:
  ‚úÖ What works well
  ‚ö†Ô∏è  What's partial
  ‚ùå What doesn't work
  üìä Accuracy score
  üéØ RAG quality score
  üí° Improvement recommendations
```

---

## ‚úÖ EXPECTED OUTCOMES

### If Everything Works Well
- ‚úÖ Accurate data retrieval (90%+ accuracy on Questions 1-44)
- ‚úÖ Good RAG synthesis (Responses combine your data + best practices)
- ‚úÖ Specific recommendations (Tailored to YOUR context)
- ‚úÖ Fast responses (< 5 seconds typically)
- üìä Ready for production use with users

### If Data Retrieval Works but RAG Doesn't
- ‚úÖ Can fetch data accurately
- ‚ö†Ô∏è Can't synthesize with best practices
- üí° Action: Enable web search, verify API keys, check KB setup

### If Neither Works Well
- ‚ùå Poor data retrieval
- ‚ùå Poor RAG capability
- üí° Action: Check database, verify user permissions, review API configuration

### Common Gaps to Document
- Specific query types that fail
- Categories where accuracy drops
- Types of questions AI can't answer
- Areas needing improvement

---

## üîç WHAT TO LOOK FOR

### Signs of Good Data Retrieval
‚úÖ Uses your actual task/board/team names  
‚úÖ Numbers match your actual data  
‚úÖ References your specific constraints  
‚úÖ Shows awareness of your org structure  
‚úÖ Clear and well-formatted response  

### Signs of Bad Data Retrieval
‚ùå Generic or hedging language ("might have", "probably")  
‚ùå Wrong names or numbers  
‚ùå Doesn't show your specific data  
‚ùå Vague responses  
‚ùå Claims inability to access  

### Signs of Good RAG
‚úÖ Mentions your specific data points  
‚úÖ References external sources or best practices  
‚úÖ Integrates both together (not just lists both)  
‚úÖ Provides tailored recommendations  
‚úÖ Shows reasoning/frameworks  
‚úÖ Actionable and specific  

### Signs of Poor RAG
‚ùå Generic advice that applies to anyone  
‚ùå Doesn't reference your data  
‚ùå Just lists web links  
‚ùå Seems like Wikipedia article  
‚ùå Can't act on the advice  
‚ùå Doesn't feel personalized  

---

## üìä RATING SCALE

### For Each Question Rate:

**Accuracy** (Data Retrieval)
- ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Perfect - Exactly matches your data
- ‚≠ê‚≠ê‚≠ê‚≠ê Good - Minor omissions, mostly accurate  
- ‚≠ê‚≠ê‚≠ê Fair - Partially accurate, some errors
- ‚≠ê‚≠ê Poor - Multiple errors or incomplete
- ‚≠ê Terrible - Wrong or can't answer

**Quality** (Strategic/RAG)
- ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellent - Specific, actionable, well-integrated
- ‚≠ê‚≠ê‚≠ê‚≠ê Good - Mostly specific, mostly integrated
- ‚≠ê‚≠ê‚≠ê Fair - Mixes specific + generic, partially integrated
- ‚≠ê‚≠ê Poor - Mostly generic, poorly integrated
- ‚≠ê Terrible - Generic only, or can't answer

---

## üéì TESTING BEST PRACTICES

### DO:
‚úÖ Test systematically, category by category  
‚úÖ Document exact questions and responses  
‚úÖ Compare to actual project data  
‚úÖ Take notes on patterns (e.g., "always fails on..." or "works great for...")  
‚úÖ Test with realistic project scenarios  
‚úÖ Note edge cases and complex queries  
‚úÖ Time response speeds  
‚úÖ Check if external sources are cited  

### DON'T:
‚ùå Test only easy questions  
‚ùå Give up on a question type without trying variations  
‚ùå Assume hallucinations (verify against your actual data)  
‚ùå Rush through categories  
‚ùå Skip the RAG questions  
‚ùå Accept vague responses without clarification  
‚ùå Test in isolation (context matters)  

---

## üõ†Ô∏è TROUBLESHOOTING

### Issue: AI says it doesn't have access to your data
**Solution**:
- Verify user is in organization and board
- Check database for demo data
- Verify user permissions are set correctly
- Check logs for errors

### Issue: Responses are very slow
**Solution**:
- Check AI API keys are valid
- Monitor token usage
- Check network connectivity
- Consider enabling caching

### Issue: AI makes up data (hallucination)
**Solution**:
- This is a failure - document it
- Note which data it hallucinated
- Check if it's a specific pattern
- Consider as a limitation

### Issue: Web search isn't working (for RAG)
**Solution**:
- Check `ENABLE_WEB_SEARCH=True` in .env
- Verify Google Search API keys
- Check quota usage
- Try a clear "latest trends" search query

### Issue: Responses don't reference your data
**Solution**:
- Check that context builders are working
- Verify user-board associations
- Test with simpler query first
- Check if model selection matters

---

## üìà CREATING A TEST REPORT

### Template for your report:

```
# AI Assistant Testing Report
Date: [Date]
Tester: [Your Name]
Test Duration: [Time]

## Executive Summary
[1-2 paragraphs on overall capability]

## Part 1: Data Retrieval Results
- Questions 1-44 tested: [Number passed]
- Accuracy: [Average %]
- Strong areas: [Categories that work well]
- Weak areas: [Categories with issues]

## Part 2: RAG/Strategic Questions Results  
- Questions 45-83 tested: [Number passed]
- RAG Quality: [Assessment]
- Best performing categories: [Which topics]
- Needs improvement: [Which topics]

## Issue Documentation
[Specific failures and hallucinations noted]

## Recommendations
[What to fix/improve next]

## Detailed Results
[Full question-by-question breakdown]
```

---

## üéØ SUCCESS CRITERIA

Consider your AI Assistant **Ready for Production** when:

‚úÖ Data Retrieval: 85%+ accuracy across all categories  
‚úÖ RAG Capability: 75%+ of responses integrate data + best practices  
‚úÖ Response Quality: Clear, specific, actionable  
‚úÖ Speed: Responses in < 5 seconds  
‚úÖ Reliability: Consistent (not random failures)  

---

## üìû NEXT STEPS AFTER TESTING

1. **Analyze results** - Which areas work, which need work?
2. **Prioritize improvements** - What has highest impact?
3. **Plan fixes** - Which issues to tackle first?
4. **Implement** - Update context builders, fix bugs
5. **Retest** - Verify improvements worked
6. **Document** - Create internal guide on AI capabilities for users
7. **Train users** - Show how to ask effective questions
8. **Monitor** - Track quality over time

---

## üìñ HOW THESE DOCUMENTS WORK TOGETHER

```
START HERE: This file (Master Index)
    ‚Üì
Want quick overview? ‚Üí AI_ASSISTANT_TESTING_QUICK_REFERENCE.md
    ‚Üì
Want to understand examples? ‚Üí AI_ASSISTANT_TESTING_EXAMPLES.md
    ‚Üì
Ready for detailed testing? ‚Üí AI_ASSISTANT_TEST_GUIDE_COMPREHENSIVE.md
    ‚Üì
Systematically test all 83 questions
    ‚Üì
Create test report with findings
    ‚Üì
Identify improvement areas
    ‚Üì
Plan and implement fixes
```

---

## ‚ö° QUICK COMMAND REFERENCE

### Running the AI Assistant
```bash
# From TaskFlow directory
python manage.py runserver

# Open browser to
http://localhost:8000/assistant/welcome/
```

### Loading Demo Data
```bash
python manage.py loaddata demo_data.json
```

### Checking Logs
```bash
tail -f logs/chatbot.log
```

### Running Tests Programmatically
```bash
python manage.py shell
from ai_assistant.utils.chatbot_service import TaskFlowChatbotService
service = TaskFlowChatbotService(user=request.user)
response = service.get_response("Your test question")
print(response)
```

---

## üí° TIPS FOR SUCCESSFUL TESTING

### Tip 1: Start with simple questions first
- Begin with Beginner level questions in each category
- Progress to Intermediate and Advanced
- Helps you understand AI's baseline capability

### Tip 2: Have real data to compare against
- Know your actual project statistics
- Open admin interface to verify data
- Take screenshots of real data for comparison

### Tip 3: Ask follow-up questions
- If AI's answer is vague, ask for clarification
- Ask "How did you get that number?" to see reasoning
- Follow-up questions often reveal what's working

### Tip 4: Test in context
- Create a realistic scenario (e.g., "Help with Q4 product board")
- Ask multiple related questions
- See if AI remembers context

### Tip 5: Document as you go
- Don't wait until end to document
- Take notes during testing
- Note exact questions asked and responses
- Makes it easier to debug issues later

---

## üéÅ BONUS: Testing Your Test Data

Before testing the AI, verify your data is set up:

Questions to check:
- [ ] Do you have at least 1 organization?
- [ ] Do you have at least 2-3 boards?
- [ ] Does each board have 5+ tasks?
- [ ] Are tasks assigned to different people?
- [ ] Do some tasks have risks/priorities set?
- [ ] Are there task dependencies?
- [ ] Do you have team members on each board?

**Why**: This ensures the AI has data to retrieve

---

## ‚ú® YOU'RE READY!

You now have everything needed to thoroughly test your AI Assistant:

üìÑ **4 comprehensive documents** with 83 test questions  
üéØ **Clear evaluation criteria** for what's working  
üîç **Examples** showing good vs bad responses  
‚úÖ **Checklists** for systematic testing  
üìä **Metrics** for measuring success  

**Next step: Pick a starting point above and begin testing!**

---

**Questions?** Check the specific document that covers that topic, or create a GitHub issue if something isn't covered.

**Ready to discover how capable your AI Assistant is?** üöÄ
