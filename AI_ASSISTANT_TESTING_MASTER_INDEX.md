# AI Assistant Testing - Master Index & Implementation Guide

**Created**: November 5, 2025  
**Purpose**: Comprehensive guide to testing your AI Assistant's capabilities

---

## 📚 DOCUMENTATION PROVIDED

You now have **4 comprehensive testing documents**:

### 1. **AI_ASSISTANT_TEST_GUIDE_COMPREHENSIVE.md** (Main Guide)
- **What**: 83 detailed test questions organized by complexity and domain
- **Length**: ~400 lines, very detailed
- **Best for**: Complete, systematic testing with full context
- **Structure**:
  - Part 1: Data Retrieval Questions (44 questions)
  - Part 2: Strategic Questions - RAG Capability (39 questions)
  - Quality metrics and troubleshooting

**Start here if**: You want the most detailed, thorough testing approach

---

### 2. **AI_ASSISTANT_TESTING_QUICK_REFERENCE.md** (Quick Start)
- **What**: Summary of testing approach with quick verification tests
- **Length**: ~250 lines, condensed
- **Best for**: Quick overview and 5-minute verification tests
- **Key sections**:
  - Overview of 83 questions
  - Quick verification tests (5 min each)
  - Expected capabilities checklist
  - Testing metrics

**Start here if**: You want a fast overview before diving into detailed testing

---

### 3. **AI_ASSISTANT_TESTING_EXAMPLES.md** (Learning Examples)
- **What**: Realistic example Q&A with evaluation guidance
- **Length**: ~350 lines with detailed examples
- **Best for**: Understanding what good vs bad responses look like
- **Includes**:
  - 6 detailed example walkthroughs
  - Expected vs actual responses
  - How to recognize when RAG is working
  - Response evaluation checklist

**Start here if**: You want to calibrate expectations and learn what to look for

---

### 4. **AI_ASSISTANT_TESTING_QUICK_REFERENCE.md** (This file - Master Index)
- **What**: Navigation guide and quick implementation checklist
- **Best for**: Finding what you need and getting started
- **Use**: As reference to jump to other documents

---

## 🎯 QUICK START (5 MINUTES)

### If you have 5 minutes:
1. Read this section (you're doing it now!)
2. Open `AI_ASSISTANT_TESTING_QUICK_REFERENCE.md`
3. Check the "Quick Verification Tests" section
4. Ask your AI the 5 quick data retrieval questions
5. Ask your AI the 3 quick RAG questions
6. See if basic capabilities work

### If you have 30 minutes:
1. Read `AI_ASSISTANT_TESTING_QUICK_REFERENCE.md` (entire)
2. Read `AI_ASSISTANT_TESTING_EXAMPLES.md` Examples 1-3
3. Start testing with categories 1A and 1B from the comprehensive guide
4. Note what works and what doesn't

### If you have 2+ hours:
1. Read all documentation in this order:
   - `AI_ASSISTANT_TESTING_QUICK_REFERENCE.md`
   - `AI_ASSISTANT_TESTING_EXAMPLES.md`
   - `AI_ASSISTANT_TEST_GUIDE_COMPREHENSIVE.md`
2. Systematically test all 83 questions
3. Create a detailed test report
4. Identify improvement areas

---

## 📋 QUESTION CATEGORIES AT A GLANCE

### PART 1: DATA RETRIEVAL (44 Questions)
These test if the AI can accurately fetch and report your project data.

| Category | Questions | Topics | Difficulty |
|----------|-----------|--------|-----------|
| **Organizations** | 1-7 | Org listing, counting, comparisons | Beginner → Advanced |
| **Boards** | 8-16 | Board data, task counts, teams | Beginner → Advanced |
| **Tasks** | 17-28 | Task filtering, status, dependencies | Beginner → Advanced |
| **Teams & Resources** | 29-36 | Team composition, workload, capacity | Beginner → Advanced |
| **Risk & Status** | 37-44 | Risk identification, indicators, analysis | Beginner → Advanced |

**Goal**: Verify AI can accurately retrieve and report on your project data

---

### PART 2: STRATEGIC QUESTIONS (39 Questions)
These test if the AI can combine your project data with best practices (RAG capability).

| Category | Questions | Topics | Focus |
|----------|-----------|--------|-------|
| **Risk Strategies** | 45-53 | How to handle risks, mitigation | Data + Best Practices |
| **PM Strategy** | 54-61 | Project delivery, team optimization | Your Projects + Frameworks |
| **Organizational** | 62-66 | Org structure, alignment, scaling | Your Structure + Strategy |
| **Process Improvement** | 67-72 | Metrics, delays, communication | Your Current State + Improvements |
| **Strategic Decisions** | 73-78 | Hiring, prioritization, tool choices | Your Constraints + Decisions |
| **Trends & Forecasting** | 79-83 | Market trends, forecasts, planning | Your Data + External Trends |

**Goal**: Verify AI blends YOUR specific data with general best practices to give tailored advice

---

## 🚀 TESTING WORKFLOW

### Phase 1: Setup (10 minutes)
```
✅ Ensure AI assistant is running
✅ Open chat interface
✅ Load demo data (if not already loaded)
✅ Have a test user logged in
✅ Open these documents in reference window
```

### Phase 2: Data Retrieval Testing (30-45 minutes)
```
Questions to ask: 1-44 (organized by category)
For each question:
  1. Ask the question
  2. Check accuracy against your actual project data
  3. Rate: ✅ Correct, ⚠️ Partial, ❌ Wrong
  4. Note response time and any issues
```

### Phase 3: RAG Testing (45-60 minutes)
```
Questions to ask: 45-83 (organized by strategy type)
For each question look for:
  🌐 Web search mention? (Industry research, trends, best practices)
  📊 Your data reference? (Your specific tasks, teams, metrics)
  🧪 Good synthesis? (Blended together vs separate lists)
  ⚡ Actionable? (Can you actually implement the advice?)
```

### Phase 4: Report & Analysis (15-20 minutes)
```
Document findings:
  ✅ What works well
  ⚠️  What's partial
  ❌ What doesn't work
  📊 Accuracy score
  🎯 RAG quality score
  💡 Improvement recommendations
```

---

## ✅ EXPECTED OUTCOMES

### If Everything Works Well
- ✅ Accurate data retrieval (90%+ accuracy on Questions 1-44)
- ✅ Good RAG synthesis (Responses combine your data + best practices)
- ✅ Specific recommendations (Tailored to YOUR context)
- ✅ Fast responses (< 5 seconds typically)
- 📊 Ready for production use with users

### If Data Retrieval Works but RAG Doesn't
- ✅ Can fetch data accurately
- ⚠️ Can't synthesize with best practices
- 💡 Action: Enable web search, verify API keys, check KB setup

### If Neither Works Well
- ❌ Poor data retrieval
- ❌ Poor RAG capability
- 💡 Action: Check database, verify user permissions, review API configuration

### Common Gaps to Document
- Specific query types that fail
- Categories where accuracy drops
- Types of questions AI can't answer
- Areas needing improvement

---

## 🔍 WHAT TO LOOK FOR

### Signs of Good Data Retrieval
✅ Uses your actual task/board/team names  
✅ Numbers match your actual data  
✅ References your specific constraints  
✅ Shows awareness of your org structure  
✅ Clear and well-formatted response  

### Signs of Bad Data Retrieval
❌ Generic or hedging language ("might have", "probably")  
❌ Wrong names or numbers  
❌ Doesn't show your specific data  
❌ Vague responses  
❌ Claims inability to access  

### Signs of Good RAG
✅ Mentions your specific data points  
✅ References external sources or best practices  
✅ Integrates both together (not just lists both)  
✅ Provides tailored recommendations  
✅ Shows reasoning/frameworks  
✅ Actionable and specific  

### Signs of Poor RAG
❌ Generic advice that applies to anyone  
❌ Doesn't reference your data  
❌ Just lists web links  
❌ Seems like Wikipedia article  
❌ Can't act on the advice  
❌ Doesn't feel personalized  

---

## 📊 RATING SCALE

### For Each Question Rate:

**Accuracy** (Data Retrieval)
- ⭐⭐⭐⭐⭐ Perfect - Exactly matches your data
- ⭐⭐⭐⭐ Good - Minor omissions, mostly accurate  
- ⭐⭐⭐ Fair - Partially accurate, some errors
- ⭐⭐ Poor - Multiple errors or incomplete
- ⭐ Terrible - Wrong or can't answer

**Quality** (Strategic/RAG)
- ⭐⭐⭐⭐⭐ Excellent - Specific, actionable, well-integrated
- ⭐⭐⭐⭐ Good - Mostly specific, mostly integrated
- ⭐⭐⭐ Fair - Mixes specific + generic, partially integrated
- ⭐⭐ Poor - Mostly generic, poorly integrated
- ⭐ Terrible - Generic only, or can't answer

---

## 🎓 TESTING BEST PRACTICES

### DO:
✅ Test systematically, category by category  
✅ Document exact questions and responses  
✅ Compare to actual project data  
✅ Take notes on patterns (e.g., "always fails on..." or "works great for...")  
✅ Test with realistic project scenarios  
✅ Note edge cases and complex queries  
✅ Time response speeds  
✅ Check if external sources are cited  

### DON'T:
❌ Test only easy questions  
❌ Give up on a question type without trying variations  
❌ Assume hallucinations (verify against your actual data)  
❌ Rush through categories  
❌ Skip the RAG questions  
❌ Accept vague responses without clarification  
❌ Test in isolation (context matters)  

---

## 🛠️ TROUBLESHOOTING

### Issue: AI says it doesn't have access to your data
**Solution**:
- Verify user is in organization and board
- Check database for demo data
- Verify user permissions are set correctly
- Check logs for errors

### Issue: Responses are very slow
**Solution**:
- Check AI API keys are valid
- Monitor token usage
- Check network connectivity
- Consider enabling caching

### Issue: AI makes up data (hallucination)
**Solution**:
- This is a failure - document it
- Note which data it hallucinated
- Check if it's a specific pattern
- Consider as a limitation

### Issue: Web search isn't working (for RAG)
**Solution**:
- Check `ENABLE_WEB_SEARCH=True` in .env
- Verify Google Search API keys
- Check quota usage
- Try a clear "latest trends" search query

### Issue: Responses don't reference your data
**Solution**:
- Check that context builders are working
- Verify user-board associations
- Test with simpler query first
- Check if model selection matters

---

## 📈 CREATING A TEST REPORT

### Template for your report:

```
# AI Assistant Testing Report
Date: [Date]
Tester: [Your Name]
Test Duration: [Time]

## Executive Summary
[1-2 paragraphs on overall capability]

## Part 1: Data Retrieval Results
- Questions 1-44 tested: [Number passed]
- Accuracy: [Average %]
- Strong areas: [Categories that work well]
- Weak areas: [Categories with issues]

## Part 2: RAG/Strategic Questions Results  
- Questions 45-83 tested: [Number passed]
- RAG Quality: [Assessment]
- Best performing categories: [Which topics]
- Needs improvement: [Which topics]

## Issue Documentation
[Specific failures and hallucinations noted]

## Recommendations
[What to fix/improve next]

## Detailed Results
[Full question-by-question breakdown]
```

---

## 🎯 SUCCESS CRITERIA

Consider your AI Assistant **Ready for Production** when:

✅ Data Retrieval: 85%+ accuracy across all categories  
✅ RAG Capability: 75%+ of responses integrate data + best practices  
✅ Response Quality: Clear, specific, actionable  
✅ Speed: Responses in < 5 seconds  
✅ Reliability: Consistent (not random failures)  

---

## 📞 NEXT STEPS AFTER TESTING

1. **Analyze results** - Which areas work, which need work?
2. **Prioritize improvements** - What has highest impact?
3. **Plan fixes** - Which issues to tackle first?
4. **Implement** - Update context builders, fix bugs
5. **Retest** - Verify improvements worked
6. **Document** - Create internal guide on AI capabilities for users
7. **Train users** - Show how to ask effective questions
8. **Monitor** - Track quality over time

---

## 📖 HOW THESE DOCUMENTS WORK TOGETHER

```
START HERE: This file (Master Index)
    ↓
Want quick overview? → AI_ASSISTANT_TESTING_QUICK_REFERENCE.md
    ↓
Want to understand examples? → AI_ASSISTANT_TESTING_EXAMPLES.md
    ↓
Ready for detailed testing? → AI_ASSISTANT_TEST_GUIDE_COMPREHENSIVE.md
    ↓
Systematically test all 83 questions
    ↓
Create test report with findings
    ↓
Identify improvement areas
    ↓
Plan and implement fixes
```

---

## ⚡ QUICK COMMAND REFERENCE

### Running the AI Assistant
```bash
# From PrizmAI directory
python manage.py runserver

# Open browser to
http://localhost:8000/assistant/welcome/
```

### Loading Demo Data
```bash
python manage.py loaddata demo_data.json
```

### Checking Logs
```bash
tail -f logs/chatbot.log
```

### Running Tests Programmatically
```bash
python manage.py shell
from ai_assistant.utils.chatbot_service import PrizmAIChatbotService
service = PrizmAIChatbotService(user=request.user)
response = service.get_response("Your test question")
print(response)
```

---

## 💡 TIPS FOR SUCCESSFUL TESTING

### Tip 1: Start with simple questions first
- Begin with Beginner level questions in each category
- Progress to Intermediate and Advanced
- Helps you understand AI's baseline capability

### Tip 2: Have real data to compare against
- Know your actual project statistics
- Open admin interface to verify data
- Take screenshots of real data for comparison

### Tip 3: Ask follow-up questions
- If AI's answer is vague, ask for clarification
- Ask "How did you get that number?" to see reasoning
- Follow-up questions often reveal what's working

### Tip 4: Test in context
- Create a realistic scenario (e.g., "Help with Q4 product board")
- Ask multiple related questions
- See if AI remembers context

### Tip 5: Document as you go
- Don't wait until end to document
- Take notes during testing
- Note exact questions asked and responses
- Makes it easier to debug issues later

---

## 🎁 BONUS: Testing Your Test Data

Before testing the AI, verify your data is set up:

Questions to check:
- [ ] Do you have at least 1 organization?
- [ ] Do you have at least 2-3 boards?
- [ ] Does each board have 5+ tasks?
- [ ] Are tasks assigned to different people?
- [ ] Do some tasks have risks/priorities set?
- [ ] Are there task dependencies?
- [ ] Do you have team members on each board?

**Why**: This ensures the AI has data to retrieve

---

## ✨ YOU'RE READY!

You now have everything needed to thoroughly test your AI Assistant:

📄 **4 comprehensive documents** with 83 test questions  
🎯 **Clear evaluation criteria** for what's working  
🔍 **Examples** showing good vs bad responses  
✅ **Checklists** for systematic testing  
📊 **Metrics** for measuring success  

**Next step: Pick a starting point above and begin testing!**

---

**Questions?** Check the specific document that covers that topic, or create a GitHub issue if something isn't covered.

**Ready to discover how capable your AI Assistant is?** 🚀
